{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a3a280-1b72-4c3f-84c1-e13170e6371a",
   "metadata": {},
   "source": [
    "# Build your first Neural Network from scratch\n",
    "## This Notebook is intended to guide you trough the construction of a simple fully connected Neural Network without using specific deeplearning frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dc6e6-b26a-4be7-bc5a-bed352a153ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import test_cases\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfc850fc-d623-41ae-9595-e5d6457abb12",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Network (FCNN) Architecture\n",
    "\n",
    "<div>\n",
    "    <img src =\"Images/NN_architecture.png\" width = \"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f1181-40e6-4206-baee-71606ffb1c71",
   "metadata": {},
   "source": [
    "## Weights and Bias \n",
    "each layer of the Neural Network is composed by a Matrix of weights and a bias vector, this hepls to simplify the implemnetation\\\n",
    "the shape of the weigth matrix is [Current_Layer_Nodes, Prev_Layer_Nodes]\\\n",
    "and the bias is [Current_Layer_Nodes, 1]\n",
    "\n",
    "the first step in the construction of our Neural Network is initialize the weigths and biases for each layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249076be-2520-48a0-996b-1bd5c25b3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the following function to initialize the weigths and bias for one layer, remember check the proper shape of the elements\n",
    "# randomly inilize the elements using a Gaussian distributions with mean 0 and standard deviation 1\n",
    "def init_layer(prev_layer:int, curr_layer:int) ->list[np.array]:\n",
    "    weigths = #your code goes here\n",
    "    bias = #your code goes here\n",
    "    return weigths,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fcc22-1033-4bea-925d-2de67edcd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the following cell to test your function\n",
    "resul=test_cases.run_check(check_name='test_init_layer',func=init_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3bcec5-a387-4779-af4a-e9f3c0d40572",
   "metadata": {},
   "source": [
    "## Layer feed forward\n",
    "<div>\n",
    "    <img src =\"Images/NN_layer.png\" width = \"500\">\n",
    "</div>\n",
    "\n",
    "the output of each layer in a FCNN can be calculated in two steps:\\\n",
    "first calculate the preactivation (z) using the formula, $z = w^T x +b$\n",
    "and pass hte result trough some activation function $g(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9493eb4-53aa-45d1-81c3-b1f982fef9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the following function to calculate the preactivation (z) calultaion for one layer\n",
    "def layer_preactivation(weights:np.array,bias:np.array,x:np.array)->np.array:\n",
    "    z = #your code goes here\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492688a8-ba28-4330-a32a-c47ccd2e146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the following cell to test your function\n",
    "resul=test_cases.run_check(check_name='test_preactivation',func=layer_preactivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e2f53-0576-4ea7-b9b1-f4b928d5fb81",
   "metadata": {},
   "source": [
    "## activation functions examples\n",
    "\n",
    "<div>\n",
    "    <img src =\"Images/activation_functions.png\" width = \"700\">\n",
    "</div>\n",
    "\n",
    "The activation function is a mathematical function that is used within neural networks and decides whether a neuron is activated or not.\\\n",
    "It processes the weighted sum of the neuronâ€™s inputs and calculates a new value to determine how strongly the signal is passed on to the next layer\\\n",
    "in the network, for this notebook we will use ReLu and Sigmoid activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea7a72-3a8d-4e51-8afc-07408e120174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement ReLU activation function\n",
    "def relu(x: float)->float:\n",
    "    g = #your code goes here\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de668837-4e8a-4ffe-82c9-ed951673f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the following cell to test your function\n",
    "resul=test_cases.run_check(check_name='test_relu',func=relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a85fcf-833f-47e3-a994-3a6b3f97f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement ReLu Prime function (relu_derivative)\n",
    "def relu_prime(x: float)->float:\n",
    "    gp = #your code goes here\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed9f4d-cbc7-4718-8133-051fc1f61736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the following cell to test your function\n",
    "resul=test_cases.run_check(check_name='test_relu_prime',func=relu_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f8b58-01e3-441d-b341-a23d802b58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Sigmoid activation function\n",
    "def sigmoid(x:float)->float:\n",
    "    g = #your code goes here\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec4420-dfc9-460c-ab10-a2f05ce8fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the following cell to test your function\n",
    "resul=test_cases.run_check(check_name='test_sigmoid',func=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed6cc2-a7b4-4c6a-9079-7ab247eadcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement Sigmoid Prime function (sigmoid_derivative)\n",
    "def sigmoid_prime(x:float)->float:\n",
    "    gp=#your code goes here\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c736a78-3209-40d1-b2b6-c3cfbefee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the following cell to test your function\n",
    "resul=test_cases.run_check(check_name='test_sigmoid_prime',func=sigmoid_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c6d82-30f8-44ac-9db0-ebaaab665a53",
   "metadata": {},
   "source": [
    "# **Train a Neural Network with Gradient Descent** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e3149-7698-474e-a06a-2e17addf3d73",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "1. **Feedforward:** compute preactivation and activation for each layer \n",
    "2. **Calculate Output error:** calculate error in the final layer using a cost function\n",
    "3. **Propagate Error:** use back propagation to estimate the contribution of each weight to the final error\n",
    "4. **Update Weights:** update weigths and bias using the following equations:\n",
    "    - $w = w - \\alpha *\\delta^ w$\n",
    "    - $b = b - \\alpha *\\delta^ b$\n",
    "      \n",
    "repeat the algorithm for a fixed number of epochs or until some condition is met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537652e-cd9d-497b-811f-709ca973d33f",
   "metadata": {},
   "source": [
    "## back propagation equations:\n",
    "$\\large \\delta^L = \\nabla_a C \\odot \\sigma'(z^L).$ \n",
    "\n",
    "$\\large  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) $\n",
    "\n",
    "$\\large  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j. $\n",
    "\n",
    "$\\large  \\frac{\\partial C}{\\partial b^l_{j}} = \\delta^l_j. $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8351d5a-5538-444f-801a-cbf43af0ead2",
   "metadata": {},
   "source": [
    "## Now Lets use all the blocks to create the simple_nn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824f4c0-55d3-4920-bacc-131a80409cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple FCNN to use create an object and pass the layers configuration  and the activations list for example\n",
    "# my_nn = simple_nn([5,4,2],['relu','sigmoid']) this will create a NN with 3 layers, each number is the number of neurons in the layer\n",
    "# the hiden layeer will use relu activation and the output layer will use sigmoid\n",
    "\n",
    "map_act   = {'relu': relu, 'sigmoid':sigmoid}\n",
    "map_prime = {'relu': relu_prime, 'sigmoid':sigmoid_prime}\n",
    "\n",
    "class simple_nn():\n",
    "    def __init__(self,layers, activations):\n",
    "        self.num_layers=len(layers) # save NN structure\n",
    "        self.activations = activations # list of activation function per layer, except input layer\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        for prev_layer, curr_layer in zip(layers,layers[1:]):\n",
    "            w,b= init_layer(prev_layer, curr_layer)\n",
    "            self.weights.append(w)\n",
    "            self.bias.append(b)\n",
    "\n",
    "    def forward_propagation(self,input_vector):\n",
    "        #forward propagation, hidden layers use ReLU activation function and ouput layer use sigmoid activation function\n",
    "        #loop trough intermediate layers\n",
    "        x=input_vector\n",
    "        for i in range(self.num_layers-1):\n",
    "            #calculate preactivation\n",
    "            z=layer_preactivation(self.weights[i],self.bias[i],x)\n",
    "            #calcute activation \n",
    "            g= map_act[self.activations[i]]\n",
    "            a=np.vectorize(g)(z)\n",
    "            x=a # update next layer input \n",
    "\n",
    "        return x\n",
    "    \n",
    "    '''\n",
    "    back propagation for cuadratic cost function  \" C=(1/2)*(y-out)**2 \"\n",
    "    return \"delta_b\" and \"delta_w\" representing the gradient propagation trough each layer\n",
    "    this variables have the same dimensions than self.weights and self.bias\n",
    "    '''\n",
    "    def backward_propagation(self,x,y):\n",
    "        #feed forward and save outputs\n",
    "        layers_z=[x]\n",
    "        layers_a=[x] \n",
    "     \n",
    "        for i in range(self.num_layers-1):\n",
    "            #calculate preactivation\n",
    "            z=layer_preactivation(self.weights[i],self.bias[i],x)\n",
    "            #calcute activation \n",
    "            g= map_act[self.activations[i]]\n",
    "            a=np.vectorize(g)(z)\n",
    "            #strore output and activation for each layer\n",
    "            layers_z.append(z)\n",
    "            layers_a.append(a)\n",
    "            x=a # update next layer input \n",
    "  \n",
    "        #initialize variables to store error by layer\n",
    "        delta_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #calculate last layer error and gradient\n",
    "        cost_gradient = (layers_a[-1]-y) # cost function gradient \n",
    "        gp= map_prime[self.activations[i-1]]\n",
    "        delta = cost_gradient*np.vectorize(gp)(layers_z[-1])\n",
    "        delta_w[-1] = np.dot(delta,layers_a[-2].transpose())\n",
    "        delta_b[-1] = delta\n",
    "        #propagate error trough hiden layers\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = layers_z[-l]\n",
    "            gp= map_prime[self.activations[-l]]\n",
    "            rp = np.vectorize(gp)(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) *rp\n",
    "            delta_b[-l] = delta\n",
    "            delta_w[-l] = np.dot(delta, layers_a[-l-1].transpose())\n",
    "\n",
    "        cost= 0.5*(layers_a[-1]-y)**2 #quadratic cost function\n",
    "        \n",
    "        return delta_b,delta_w,cost\n",
    "\n",
    "    \n",
    "    def update_minibatch(self,x,y,alpha):\n",
    "        #update weigths and bias  for a minibatch (array of x and y pairs)\n",
    "        #alpha is the learning rate\n",
    "\n",
    "        #initialize variables to store error\n",
    "        delta_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        cost=0\n",
    "        #iterate trough pairs\n",
    "        m = len(x) #number of samples in batch\n",
    "        for i in range(m):\n",
    "            #calculate delta for each x,y pair \n",
    "            xi=  x[[i]].T #slice array and transpose (size = (inputs,1))\n",
    "            yi = y[i]  \n",
    "            dbi,dwi,cost_i= self.backward_propagation(xi,yi)\n",
    "            #sum error for each pair in minibatch\n",
    "            delta_b = [db+dnb for db, dnb in zip(delta_b, dbi)]\n",
    "            delta_w = [dw+dnw for dw, dnw in zip(delta_w, dwi)]\n",
    "\n",
    "            cost+=cost_i\n",
    "\n",
    "            \n",
    "        #update weights using learning rate (alpha) divided by the number of elements in minibatch\n",
    "        \n",
    "        self.weights = [w - (alpha/m)*nw for w,nw in zip(self.weights,delta_w)]\n",
    "        self.bias    = [b - (alpha/m)*nb for b,nb in zip(self.bias,delta_b)]\n",
    "\n",
    "        return cost/m          \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e772f-3e3c-4eac-a5c6-359f8316eeb6",
   "metadata": {},
   "source": [
    "## We will test and train our new neural network using some artificial data generated using [scikit make_moons](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd4f4e6-f605-4d1e-95f7-0c7636ee4f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fig,ax  = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
    "\n",
    "X, Y = make_moons(noise=0.1, random_state=0,n_samples = 200)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.suptitle(\"Moons Dataset\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd88b4-05b2-4358-b91d-1a393e983a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for back propagation and update minibatch is provided to you in the NN class,\n",
    "# lets use this to train the Neural Network, t\n",
    "def train_nn(NN:simple_nn , X_train:np.array, Y_train:np.array,batch_size, alpha : float, epochs:int):\n",
    "    n_batchs = len(X_train)//batch_size # calculate number of batchs\n",
    "    logs=[]\n",
    "    for it in range(epochs):\n",
    "        cost = 0 # restart cost for every epoch\n",
    "        for i in range(n_batchs):\n",
    "            #create mini batch\n",
    "            xb = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            yb = Y_train[i*batch_size:(i+1)*batch_size] \n",
    "            # update weigths for the minibatch\n",
    "            batch_cost = NN.update_minibatch(x=xb,y=yb,alpha=alpha) \n",
    "            cost += batch_cost.item()\n",
    "        #calculate training metrics\n",
    "        cost = cost/n_batchs\n",
    "        y_pred =1*(NN.forward_propagation(X_train.T)>0.5)\n",
    "        train_accuracy = np.sum(y_pred==Y_train) /len(Y_train)\n",
    "        logs.append([cost,train_accuracy]) #save all costs\n",
    "\n",
    "        #print every 20 epochs\n",
    "        if(it%20 == 0): \n",
    "            print (\"epoch {} finished, train_cost = {:0.3f}, train_accuracy = {:0.3f} \".format(it,cost,train_accuracy))\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027aadd4-e4a4-4f8a-aaa2-35c8a2f3eaa8",
   "metadata": {},
   "source": [
    "### Lets create a NN with the following configuration:\n",
    "   - 2 neurons in the input layer\n",
    "   - 8 neurons in the hidden layer, activation 'relu'\n",
    "   - 1 neuron in the output layer, activation 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db6498-9d99-4aed-92ca-6f1a0dfa5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = simple_nn([2,8,1],['relu','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0863ae-a179-472e-9423-50ce91a944fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset in test and train 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "logs = train_nn(my_nn,X_train,y_train,batch_size=10,alpha=0.01,epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e8276-5ac6-4bda-a224-f11972757dac",
   "metadata": {},
   "source": [
    "# PLot Neural Network Training Perormance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539157e-08b9-41ec-adaf-e77b12f7acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5392697-bd02-4176-85bf-1bdfdd558860",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =1*(my_nn.forward_propagation(X_test.T)>0.5)\n",
    "CM =confusion_matrix(y_test.T,y_pred.T)\n",
    "disp = ConfusionMatrixDisplay(CM)\n",
    "disp.plot(cmap=plt.cm.Blues,colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b6c67-aa4b-4aae-bae4-02318904c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot Neural Network Learning curves\n",
    "fig,(ax1,ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "data = np.asarray(logs)\n",
    "\n",
    "ax1.plot(data[:,0],linestyle = 'dotted',label = 'error')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(data[:,1],linestyle = 'dashed', label ='accuracy')\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.legend()\n",
    "\n",
    "fig.suptitle(\"Training Learning Curves\")\n",
    "fig.supxlabel(\"epochs\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdfdd0d-12af-4075-ac32-db5985b50713",
   "metadata": {},
   "source": [
    "# Solutions to excersises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036b135-d97d-47d5-91b8-7d6bf8e1c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(prev_layer:int, curr_layer:int) ->list[np.array]:\n",
    "    weigths =  np.random.normal(loc= 0 ,scale= 1.0, size=(curr_layer,prev_layer))\n",
    "    bias = np.random.normal(loc= 0 ,scale= 1.0,size=(curr_layer,1))\n",
    "    return weigths,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596bf3d2-e136-4720-b050-86bc1f092424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the following function to calculate the preactivation (z) calultaion for one layer\n",
    "def layer_preactivation(weights:np.array,bias:np.array,x:np.array)->np.array:\n",
    "    z = np.dot(weights,x)+bias\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bfbdd-605c-4e0f-99bb-f55f8ad5e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement ReLU activation function\n",
    "def relu(x: float)->float:\n",
    "    g = np.maximum(0,x)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef9355-41e4-4357-ae9d-f5d7f95d3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement ReLu Prime function (relu_derivative)\n",
    "def relu_prime(x: float)->float:\n",
    "    gp = 1 if (x>0) else 0\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bbb3ad-c1dd-4976-9d19-c2977d73e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Sigmoid activation function\n",
    "def sigmoid(x:float)->float:\n",
    "    g = 1/(1+np.e**-x)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43528f1-e5fb-49f2-b8f3-c944fbab9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement Sigmoid Prime function (sigmoid_derivative)\n",
    "def sigmoid_prime(x:float)->float:\n",
    "    gp=sigmoid(x)*(1-sigmoid(x))\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe36314-0dd0-40a5-9126-0fedccce6368",
   "metadata": {},
   "source": [
    "## references:\n",
    "- https://towardsdatascience.com/activation-functions-in-neural-networks-how-to-choose-the-right-one-cb20414c04e5/\n",
    "- http://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
